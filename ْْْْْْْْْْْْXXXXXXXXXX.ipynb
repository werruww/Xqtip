{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pew7-hOW0zE5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ja4PfZdO03Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uhdyi5kN03ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Dao-AILab/fast-hadamard-transform.git"
      ],
      "metadata": {
        "id": "rw-okmAnt1sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd fast-hadamard-transform\n",
        "!pip install ."
      ],
      "metadata": {
        "id": "mo4aDL1nuTjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fast_hadamard_transform import hadamard_transform"
      ],
      "metadata": {
        "id": "tFscp_eb07Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "EzJlfxHb1TrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Cornell-RelaxML/quip-sharp.git"
      ],
      "metadata": {
        "id": "rFmIa_Ry1OAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "jbvE-ykM1ceL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "# Load the model pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"relaxml/Llama-2-7b-chat-QTIP-2Bit\", trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# Use a properly formatted string as input, with do_sample\n",
        "response = pipe(\"Who are you?\", max_new_tokens=22, do_sample=True)\n",
        "\n",
        "# Print response\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "f_f-Z4Sh12Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "# Load the model pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"relaxml/Llama-2-7b-chat-QTIP-2Bit\", trust_remote_code=True, torch_dtype=\"auto\", device_map=\"auto\")\n",
        "\n",
        "# Use a properly formatted string as input, with do_sample\n",
        "response = pipe(\"Who are you?\", max_new_tokens=22, do_sample=True)\n",
        "\n",
        "# Print response\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "APm6LN5-37Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"relaxml/Llama-2-7b-chat-QTIP-2Bit\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"relaxml/Llama-2-7b-chat-QTIP-2Bit\", trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# we use alpaca prompt\n",
        "input_content = \"hi\"\n",
        "input_ids = tokenizer.encode(input_content, return_tensors=\"pt\")\n",
        "output = model.generate(input_ids, max_length=11, temperature=0.7)\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "GRV4Loha4VdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Explicitly download the tokenizer files\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"relaxml/Llama-2-7b-chat-QTIP-2Bit\")\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"relaxml/Llama-2-7b-chat-QTIP-2Bit\", trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# The rest of your code\n",
        "input_content = \"hi\"\n",
        "input_ids = tokenizer.encode(input_content, return_tensors=\"pt\")\n",
        "output = model.generate(input_ids, max_length=11, temperature=0.7)\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dYMIVmaN4xA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# تحديد اسم النموذج\n",
        "model_name = \"relaxml/Llama-2-7b-chat-QTIP-2Bit\"\n",
        "\n",
        "# تحميل tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تحميل النموذج\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "RKPL4Xbw5Trc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lib.utils.unsafe_import import model_from_hf_path\n",
        "import torch\n",
        "\n",
        "def get_quantized_layer(path2qmodel=\"YOURPATH\"):\n",
        "    # load a quantized model\n",
        "    quant_model = model_from_hf_path(path2qmodel)[0].float()\n",
        "    # select an arbitrary layer.\n",
        "    quantized_layer = quant_model.model.layers[0].self_attn.q_proj\n",
        "\n",
        "    # replicate the routine in finetune_e2e_llama.py #L95:107 with --ft_train_lut flag\n",
        "    quantized_layer.SU = torch.nn.Parameter(quantized_layer.SU.float(), requires_grad=True)\n",
        "    quantized_layer.SV = torch.nn.Parameter(quantized_layer.SV.float(), requires_grad=True)\n",
        "    quantized_layer.mode = \"train-recons\"\n",
        "    quantized_layer.tlut.requires_grad = True\n",
        "\n",
        "    return quantized_layer\n",
        "\n",
        "def test_backward():\n",
        "    # load quantized layer\n",
        "    quantized_layer = get_quantized_layer()\n",
        "\n",
        "    # initialize random input to the layer\n",
        "    ft_bs, ctx_size, in_features = 4, 4096, 4096\n",
        "    input = torch.randn(ft_bs, ctx_size, in_features).to('cuda').to(torch.float16)\n",
        "    input.requires_grad = True\n",
        "\n",
        "    print(\"=== Before backward ===\")\n",
        "    print(\"input\", input.grad)\n",
        "    print(\"SU\", quantized_layer.SU.grad)\n",
        "    print(\"SV\", quantized_layer.SV.grad)\n",
        "    print(\"tlut\", quantized_layer.tlut.grad)\n",
        "\n",
        "    # forward pass\n",
        "    output = quantized_layer(input)\n",
        "\n",
        "    # backward pass\n",
        "    loss = output.sum()\n",
        "    loss.backward()\n",
        "\n",
        "    print(\"=== After backward ===\")\n",
        "    print(\"input\", input.grad)\n",
        "    print(\"SU\", quantized_layer.SU.grad)\n",
        "    print(\"SV\", quantized_layer.SV.grad)\n",
        "    print(\"tlut\", quantized_layer.tlut.grad)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_backward()"
      ],
      "metadata": {
        "id": "2Pp1-oa-5UIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " torch_dtype=torch.float16, device_map=\"auto\""
      ],
      "metadata": {
        "id": "gensz21k67cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "# Load the model pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"relaxml/Llama-2-7b-chat-QTIP-2Bit\", trust_remote_code=True)\n",
        "\n",
        "# Use a properly formatted string as input, with do_sample\n",
        "response = pipe(\"Who are you?\", max_new_tokens=22, do_sample=True)\n",
        "\n",
        "# Print response\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "y1GdE25i63gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "assert torch.cuda.is_available(), \"CUDA غير متاح، تأكد من أنك تستخدم T4!\"\n"
      ],
      "metadata": {
        "id": "8jF17Bxk7VP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA متاح؟\", torch.cuda.is_available())\n",
        "print(\"عدد كروت الشاشة المتاحة:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"اسم كرت الشاشة:\", torch.cuda.get_device_name(0))\n",
        "    print(\"إصدار CUDA:\", torch.version.cuda)\n",
        "else:\n",
        "    print(\"⚠️ CUDA غير متاح، تأكد من تمكين GPU في كولاب!\")\n"
      ],
      "metadata": {
        "id": "uuAGu28D7Vkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"relaxml/Llama-2-7b-chat-QTIP-2Bit\"\n",
        "\n",
        "# تحميل التوكنيزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "# تحميل النموذج على CUDA\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(\"cuda\")\n",
        "\n",
        "# إدخال النص وتحويله إلى tensors\n",
        "inputs = tokenizer(\"Who are you?\", return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# توليد النص\n",
        "output = model.generate(**inputs, max_new_tokens=22, do_sample=True)\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "gyimqaut7d_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "AVnGCFSQ7uul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"mobiuslabsgmbh/Llama-3.2-3B-Instruct_4bitgs64_hqq_hf\"\n",
        "\n",
        "# تحميل التوكنيزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "# تحميل النموذج على CUDA\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(\"cuda\")\n",
        "\n",
        "# إدخال النص وتحويله إلى tensors\n",
        "inputs = tokenizer(\"Who are you?\", return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# توليد النص\n",
        "output = model.generate(**inputs, max_new_tokens=22, do_sample=True)\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "RqFRequ37wfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/interactive_gen.py"
      ],
      "metadata": {
        "id": "QTft0u5N9GM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli model info relaxml/Llama-2-7b-chat-QTIP-2Bit\n"
      ],
      "metadata": {
        "id": "npOn0q7K9P6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OG_uE5xs98eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "# Load the model pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"relaxml/Llama-1-7b-E8P-2Bit\", trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# Use a properly formatted string as input, with do_sample\n",
        "response = pipe(\"Who are you?\", max_new_tokens=22, do_sample=True)\n",
        "\n",
        "# Print response\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "mGtn6iay9_NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from lib.utils.unsafe_import import model_from_hf_path  # التأكد من توفر `lib.utils.unsafe_import`\n",
        "\n",
        "# تحقق من توافر CUDA\n",
        "assert torch.cuda.is_available(), \"CUDA غير متاح، تأكد من تفعيل GPU في كولاب!\"\n",
        "\n",
        "# تحميل النموذج من Hugging Face\n",
        "model_id = \"relaxml/Llama-2-7b-chat-QTIP-2Bit\"\n",
        "model, model_str = model_from_hf_path(model_id, use_cuda_graph=False, device_map=\"cuda:0\")\n",
        "\n",
        "# تحميل التوكنيزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# إعداد التخزين المؤقت للنموذج\n",
        "from transformers import StaticCache\n",
        "model._setup_cache(StaticCache, 1, max_cache_len=2048)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_text(prompt, max_new_tokens=100, top_k=5, temperature=0.6):\n",
        "    \"\"\" استدلال النص باستخدام النموذج \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    cache_position = torch.arange(inputs[\"input_ids\"].shape[1], device=model.device)\n",
        "\n",
        "    logits = model(**inputs, cache_position=cache_position, return_dict=False, use_cache=True)[0]\n",
        "    probs = torch.nn.functional.softmax(logits[:, -1] / temperature, dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "    generated_tokens = [next_token.item()]\n",
        "    for _ in range(max_new_tokens - 1):\n",
        "        cache_position += 1\n",
        "        logits = model(next_token, cache_position=cache_position, return_dict=False, use_cache=True)[0]\n",
        "        probs = torch.nn.functional.softmax(logits[:, -1] / temperature, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        generated_tokens.append(next_token.item())\n",
        "\n",
        "    return tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "# تجربة النموذج\n",
        "prompt = \"Who are you?\"\n",
        "output = generate_text(prompt, max_new_tokens=50, top_k=5, temperature=0.7)\n",
        "\n",
        "print(\"📝 النموذج يقول:\", output)\n"
      ],
      "metadata": {
        "id": "hSO2nXuI-CjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# تحقق من توافر CUDA\n",
        "assert torch.cuda.is_available(), \"CUDA غير متاح، تأكد من تفعيل GPU في كولاب!\"\n",
        "\n",
        "# تحميل النموذج والتوكنيزر\n",
        "model_id = \"relaxml/Llama-2-7b-chat-QTIP-2Bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(\"cuda\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_text(prompt, max_new_tokens=100, top_k=5, temperature=0.6):\n",
        "    \"\"\" استدلال النص باستخدام النموذج \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        top_k=top_k,\n",
        "        temperature=temperature,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# تجربة النموذج\n",
        "prompt = \"Who are you?\"\n",
        "output = generate_text(prompt, max_new_tokens=50, top_k=5, temperature=0.7)\n",
        "\n",
        "print(\"📝 النموذج يقول:\", output)\n"
      ],
      "metadata": {
        "id": "p_tSKyAuAEce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"relaxml/Llama-2-7b-chat-QTIP-2Bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(\"cpu\")\n",
        "\n",
        "prompt = \"Who are you?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
        "\n",
        "output_ids = model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
        "output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"📝 النموذج يقول:\", output)\n"
      ],
      "metadata": {
        "id": "NixkCrIUAFFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_path = \"/root/.cache/huggingface/hub/models--relaxml--Llama-2-7b-chat-QTIP-2Bit/snapshots/1725daafaf5c630feedeb27c9a277eba9874f417\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(\"✅ النموذج موجود في المسار:\", model_path)\n",
        "    print(\"📂 الملفات الموجودة في المجلد:\")\n",
        "    print(os.listdir(model_path))\n",
        "else:\n",
        "    print(\"❌ النموذج غير موجود، تأكد من تحميله بشكل صحيح!\")\n"
      ],
      "metadata": {
        "id": "6zf2vOR2Agi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_id = \"relaxml/Llama-2-7b-chat-QTIP-2Bit\"\n",
        "local_model_path = \"/content/relaxml_model\"\n",
        "\n",
        "# تحميل ملفات التوكنيزر الناقصة\n",
        "hf_hub_download(repo_id=model_id, filename=\"tokenizer.json\", local_dir=local_model_path)\n",
        "hf_hub_download(repo_id=model_id, filename=\"tokenizer_config.json\", local_dir=local_model_path)\n",
        "hf_hub_download(repo_id=model_id, filename=\"special_tokens_map.json\", local_dir=local_model_path)\n"
      ],
      "metadata": {
        "id": "UuHpO6RaAg0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)\n",
        "print(\"✅ تم تحميل التوكنيزر بنجاح!\")\n"
      ],
      "metadata": {
        "id": "HTd3JsSuA_23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "model_id = \"relaxml/Llama-2-7b-chat-E8PRVQ-4Bit\"\n",
        "local_model_path = \"/content/relaxml_model\"\n",
        "\n",
        "# تحميل جميع ملفات النموذج إلى المجلد\n",
        "snapshot_download(repo_id=model_id, local_dir=local_model_path)\n",
        "\n",
        "print(f\"✅ تم تحميل النموذج بالكامل إلى: {local_model_path}\")\n"
      ],
      "metadata": {
        "id": "QP1aydHQBFX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "local_model_path = \"/content/relaxml_model\"\n",
        "#model = AutoModelForCausalLM.from_pretrained(local_model_path, trust_remote_code=True).to(\"cpu\")\n",
        "\n",
        "# تحميل التوكنيزر والنموذج من المجلد\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(local_model_path, trust_remote_code=True).to(\"cpu\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_text(prompt, max_new_tokens=1, top_k=5, temperature=0.6):\n",
        "    \"\"\" استدلال النص باستخدام النموذج \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        top_k=top_k,\n",
        "        temperature=temperature,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# تجربة النموذج\n",
        "prompt = \"Who are you?\"\n",
        "output = generate_text(prompt, max_new_tokens=50, top_k=5, temperature=0.7)\n",
        "\n",
        "print(\"📝 النموذج يقول:\", output)\n"
      ],
      "metadata": {
        "id": "MAGyDDDFB6df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "local_model_path = \"/content/relaxml_model\"\n",
        "\n",
        "# تحميل التوكنيزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)\n",
        "\n",
        "# تحميل النموذج مع توزيع الأجزاء تلقائيًا بين CPU و GPU\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    local_model_path,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"  # يجعل بعض الأجزاء تعمل على CPU لتوفير VRAM\n",
        ")\n",
        "\n",
        "print(\"✅ تم تحميل النموذج بنجاح!\")\n"
      ],
      "metadata": {
        "id": "IVw5dfRHDPEa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}