{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pew7-hOW0zE5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ja4PfZdO03Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uhdyi5kN03ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Dao-AILab/fast-hadamard-transform.git"
      ],
      "metadata": {
        "id": "rw-okmAnt1sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd fast-hadamard-transform\n",
        "!pip install ."
      ],
      "metadata": {
        "id": "mo4aDL1nuTjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fast_hadamard_transform import hadamard_transform"
      ],
      "metadata": {
        "id": "tFscp_eb07Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "EzJlfxHb1TrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Cornell-RelaxML/quip-sharp.git"
      ],
      "metadata": {
        "id": "rFmIa_Ry1OAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "jbvE-ykM1ceL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "# Load the model pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"relaxml/Llama-2-7b-chat-QTIP-2Bit\", trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# Use a properly formatted string as input, with do_sample\n",
        "response = pipe(\"Who are you?\", max_new_tokens=22, do_sample=True)\n",
        "\n",
        "# Print response\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "f_f-Z4Sh12Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "# Load the model pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"relaxml/Llama-2-7b-chat-QTIP-2Bit\", trust_remote_code=True, torch_dtype=\"auto\", device_map=\"auto\")\n",
        "\n",
        "# Use a properly formatted string as input, with do_sample\n",
        "response = pipe(\"Who are you?\", max_new_tokens=22, do_sample=True)\n",
        "\n",
        "# Print response\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "APm6LN5-37Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"relaxml/Llama-2-7b-chat-QTIP-2Bit\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"relaxml/Llama-2-7b-chat-QTIP-2Bit\", trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# we use alpaca prompt\n",
        "input_content = \"hi\"\n",
        "input_ids = tokenizer.encode(input_content, return_tensors=\"pt\")\n",
        "output = model.generate(input_ids, max_length=11, temperature=0.7)\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "GRV4Loha4VdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Explicitly download the tokenizer files\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"relaxml/Llama-2-7b-chat-QTIP-2Bit\")\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"relaxml/Llama-2-7b-chat-QTIP-2Bit\", trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# The rest of your code\n",
        "input_content = \"hi\"\n",
        "input_ids = tokenizer.encode(input_content, return_tensors=\"pt\")\n",
        "output = model.generate(input_ids, max_length=11, temperature=0.7)\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dYMIVmaN4xA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# تحديد اسم النموذج\n",
        "model_name = \"relaxml/Llama-2-7b-chat-QTIP-2Bit\"\n",
        "\n",
        "# تحميل tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تحميل النموذج\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "RKPL4Xbw5Trc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lib.utils.unsafe_import import model_from_hf_path\n",
        "import torch\n",
        "\n",
        "def get_quantized_layer(path2qmodel=\"YOURPATH\"):\n",
        "    # load a quantized model\n",
        "    quant_model = model_from_hf_path(path2qmodel)[0].float()\n",
        "    # select an arbitrary layer.\n",
        "    quantized_layer = quant_model.model.layers[0].self_attn.q_proj\n",
        "\n",
        "    # replicate the routine in finetune_e2e_llama.py #L95:107 with --ft_train_lut flag\n",
        "    quantized_layer.SU = torch.nn.Parameter(quantized_layer.SU.float(), requires_grad=True)\n",
        "    quantized_layer.SV = torch.nn.Parameter(quantized_layer.SV.float(), requires_grad=True)\n",
        "    quantized_layer.mode = \"train-recons\"\n",
        "    quantized_layer.tlut.requires_grad = True\n",
        "\n",
        "    return quantized_layer\n",
        "\n",
        "def test_backward():\n",
        "    # load quantized layer\n",
        "    quantized_layer = get_quantized_layer()\n",
        "\n",
        "    # initialize random input to the layer\n",
        "    ft_bs, ctx_size, in_features = 4, 4096, 4096\n",
        "    input = torch.randn(ft_bs, ctx_size, in_features).to('cuda').to(torch.float16)\n",
        "    input.requires_grad = True\n",
        "\n",
        "    print(\"=== Before backward ===\")\n",
        "    print(\"input\", input.grad)\n",
        "    print(\"SU\", quantized_layer.SU.grad)\n",
        "    print(\"SV\", quantized_layer.SV.grad)\n",
        "    print(\"tlut\", quantized_layer.tlut.grad)\n",
        "\n",
        "    # forward pass\n",
        "    output = quantized_layer(input)\n",
        "\n",
        "    # backward pass\n",
        "    loss = output.sum()\n",
        "    loss.backward()\n",
        "\n",
        "    print(\"=== After backward ===\")\n",
        "    print(\"input\", input.grad)\n",
        "    print(\"SU\", quantized_layer.SU.grad)\n",
        "    print(\"SV\", quantized_layer.SV.grad)\n",
        "    print(\"tlut\", quantized_layer.tlut.grad)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_backward()"
      ],
      "metadata": {
        "id": "2Pp1-oa-5UIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " torch_dtype=torch.float16, device_map=\"auto\""
      ],
      "metadata": {
        "id": "gensz21k67cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "# Load the model pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"relaxml/Llama-2-7b-chat-QTIP-2Bit\", trust_remote_code=True)\n",
        "\n",
        "# Use a properly formatted string as input, with do_sample\n",
        "response = pipe(\"Who are you?\", max_new_tokens=22, do_sample=True)\n",
        "\n",
        "# Print response\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "y1GdE25i63gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "assert torch.cuda.is_available(), \"CUDA غير متاح، تأكد من أنك تستخدم T4!\"\n"
      ],
      "metadata": {
        "id": "8jF17Bxk7VP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA متاح؟\", torch.cuda.is_available())\n",
        "print(\"عدد كروت الشاشة المتاحة:\", torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"اسم كرت الشاشة:\", torch.cuda.get_device_name(0))\n",
        "    print(\"إصدار CUDA:\", torch.version.cuda)\n",
        "else:\n",
        "    print(\"⚠️ CUDA غير متاح، تأكد من تمكين GPU في كولاب!\")\n"
      ],
      "metadata": {
        "id": "uuAGu28D7Vkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"relaxml/Llama-2-7b-chat-QTIP-2Bit\"\n",
        "\n",
        "# تحميل التوكنيزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "# تحميل النموذج على CUDA\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(\"cuda\")\n",
        "\n",
        "# إدخال النص وتحويله إلى tensors\n",
        "inputs = tokenizer(\"Who are you?\", return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# توليد النص\n",
        "output = model.generate(**inputs, max_new_tokens=22, do_sample=True)\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "gyimqaut7d_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "AVnGCFSQ7uul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"mobiuslabsgmbh/Llama-3.2-3B-Instruct_4bitgs64_hqq_hf\"\n",
        "\n",
        "# تحميل التوكنيزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "# تحميل النموذج على CUDA\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).to(\"cuda\")\n",
        "\n",
        "# إدخال النص وتحويله إلى tensors\n",
        "inputs = tokenizer(\"Who are you?\", return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# توليد النص\n",
        "output = model.generate(**inputs, max_new_tokens=22, do_sample=True)\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "RqFRequ37wfU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}